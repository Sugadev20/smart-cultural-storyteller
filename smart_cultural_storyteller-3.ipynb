{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Smart Cultural Storyteller\n",
        "\n",
        "An AI-based automated storytelling system that generates short videos by combining:\n",
        "- AI-generated stories\n",
        "- AI-generated images\n",
        "- AI-generated audio narration\n",
        "- Automated video creation\n",
        "- Automatic captions"
      ],
      "metadata": {
        "id": "-LFlkshA0Idb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Problem Definition & Objective\n",
        "\n",
        "### 1.1 Selected Project Track\n",
        "\n",
        "This project falls under the **AI Applications â€“ LLM-based Multimodal Systems** track.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2 Problem Statement\n",
        "\n",
        "Most academic AI projects focus primarily on textual or numerical outputs, limiting user engagement and real-world applicability. Cultural storytellingâ€”especially mythological, ancestral, emotional, and traditional narrativesâ€”is often restricted to text or oral formats and is gradually fading among younger generations.\n",
        "\n",
        "There is a lack of automated systems that can generate culturally rich stories and present them in an engaging audiovisual format using modern AI technologies.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.3 Objective and Real-World Relevance\n",
        "\n",
        "The objective of this project is to design and implement an automated AI-based storytelling system that:\n",
        "- Generates short, unique stories based on user-selected storytelling modes\n",
        "- Converts stories into scene-wise images\n",
        "- Produces natural narration audio\n",
        "- Combines images, audio, and captions into a final video\n",
        "\n",
        "The system has real-world relevance in areas such as digital heritage preservation, education, content creation, and accessibility-focused applications."
      ],
      "metadata": {
        "id": "5cFeDaQABoFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Understanding & Preparation\n",
        "\n",
        "### 2.1 Data Source\n",
        "\n",
        "This project does not rely on a traditional static dataset such as CSV or tabular files. Instead, it uses dynamically generated data obtained through AI models and APIs.\n",
        "\n",
        "The primary data sources are:\n",
        "- User inputs (storytelling mode selection)\n",
        "- AI-generated text from a Large Language Model (Groq LLaMA 3)\n",
        "- Synthetic prompts generated for image creation\n",
        "- AI-generated images and audio narration\n",
        "\n",
        "All data is generated on-demand during runtime.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2 Data Collection Method\n",
        "\n",
        "The data is generated programmatically through the following steps:\n",
        "1. User selects a storytelling mode.\n",
        "2. The input is sent to the Groq LLaMA 3 model to generate a structured story.\n",
        "3. Scene-wise text is converted into image prompts.\n",
        "4. Generated text is used to create narration audio.\n",
        "\n",
        "No manual data collection is involved.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.3 Data Preprocessing\n",
        "\n",
        "Minimal preprocessing is required as the data is generated in a structured format.\n",
        "Preprocessing steps include:\n",
        "- Parsing story text into individual scenes\n",
        "- Cleaning extra whitespace and special characters\n",
        "- Structuring text for image, audio, and caption generation\n",
        "\n",
        "---\n",
        "\n",
        "### 2.4 Handling Noise or Missing Data\n",
        "\n",
        "Traditional missing values are not applicable in this project. Basic validation checks ensure:\n",
        "- The required number of scenes are generated\n",
        "- Scene text is not empty or malformed\n",
        "- Consistency across images, captions, and audio"
      ],
      "metadata": {
        "id": "gqtcH3LXB8Bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model / System Design\n",
        "\n",
        "### 3.1 AI Technique Used\n",
        "\n",
        "This project uses a multimodal AI approach combining:\n",
        "- Large Language Models for story generation\n",
        "- Diffusion models for image generation\n",
        "- Text-to-Speech synthesis for narration\n",
        "- Multimedia processing for video creation\n",
        "\n",
        "No models are trained from scratch.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2 System Architecture\n",
        "\n",
        "The system follows a modular pipeline architecture:\n",
        "1. User Input Module\n",
        "2. Story Generation Module\n",
        "3. Image Prompt Generation Module\n",
        "4. Image Generation Module\n",
        "5. Audio Generation Module\n",
        "6. Video and Caption Composition Module\n",
        "\n",
        "Each module operates independently, enabling scalability and easy maintenance.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.3 Model and Tool Selection\n",
        "\n",
        "- Groq API (LLaMA 3 â€“ 70B): Story generation\n",
        "- Stable Diffusion XL: Image generation\n",
        "- Edge Text-to-Speech: Audio narration\n",
        "- MoviePy: Video creation and caption overlay\n",
        "\n",
        "---\n",
        "\n",
        "### 3.4 Design Justification\n",
        "\n",
        "A modular design was chosen to simplify development and debugging.  \n",
        "Using pre-trained models reduces computational cost while ensuring high-quality outputs."
      ],
      "metadata": {
        "id": "TSQO90XDCB_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Core Implementation\n",
        "\n",
        "### 4.1 User Input and Story Generation\n",
        "\n",
        "The system begins by collecting user input for the storytelling mode.  \n",
        "The selected mode is sent as a prompt to a Large Language Model using the Groq API.\n",
        "\n",
        "The LLaMA 3 (70B) model is used to generate a short, unique story consisting of 5â€“7 scenes.  \n",
        "Each scene is generated in a structured format to support downstream image, audio, and caption generation."
      ],
      "metadata": {
        "id": "HYocrM-tCuzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from groq import Groq\n",
        "from google.colab import userdata # Import userdata for Colab Secrets\n",
        "\n",
        "# --- 1. Ask the user to choose ONE storytelling mode ---\n",
        "storytelling_modes = ['Mythological', 'Cultural', 'Emotional', 'Ancestral']\n",
        "\n",
        "while True:\n",
        "    print(\"\\nPlease choose a storytelling mode:\")\n",
        "    for i, mode in enumerate(storytelling_modes):\n",
        "        print(f\"  {i+1}. {mode}\")\n",
        "\n",
        "    user_choice = input(\"Enter the number corresponding to your chosen mode: \").strip()\n",
        "\n",
        "    try:\n",
        "        choice_index = int(user_choice) - 1\n",
        "        if 0 <= choice_index < len(storytelling_modes):\n",
        "            selected_mode = storytelling_modes[choice_index]\n",
        "            print(f\"You have selected the '{selected_mode}' storytelling mode.\\n\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid number. Please choose a number from the list.\")\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter a number.\")\n",
        "\n",
        "# --- 2. Store the selected storytelling mode as user input (already done above) ---\n",
        "\n",
        "# --- 3. Use the Groq API with the model: llama3-70b-8192 ---\n",
        "# Ensure your GROQ_API_KEY is set in Colab Secrets\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\") # Fetch API key from Colab Secrets\n",
        "\n",
        "if not groq_api_key:\n",
        "    raise ValueError(\n",
        "        \"GROQ_API_KEY not found in Colab Secrets. \"\n",
        "        \"Please add your Groq API key to Colab Secrets (ðŸ”‘ icon on the left panel) \"\n",
        "        \"under the name 'GROQ_API_KEY'.\"\n",
        "    )\n",
        "\n",
        "client = Groq(api_key=groq_api_key)\n",
        "# The model 'llama-3.1-70b-8192' was not found or accessible.\n",
        "# Please check Groq's official documentation for currently supported models and update the model_name below.\n",
        "# You can often find this information on the Groq Console or their API documentation:\n",
        "# https://console.groq.com/docs/deprecations\n",
        "model_name = \"groq/compound\" # Placeholder - **UPDATE THIS WITH A VALID MODEL NAME**\n",
        "\n",
        "# --- 4. Send a prompt to the Groq LLM ---\n",
        "# Define the system and user prompts to meet the requirements\n",
        "system_prompt = (\n",
        "    \"You are an expert storyteller. Your task is to generate a short, unique story \"\n",
        "    \"suitable for narration in a short video. The story must contain exactly 5 to 7 scenes. \"\n",
        "    \"Use simple, clear English. The story should be culturally rich and vivid, matching \"\n",
        "    \"the requested storytelling mode. \"\n",
        "    \"The response MUST strictly follow the format: 'Scene 1: <text>\\nScene 2: <text>\\n...'\"\n",
        ")\n",
        "\n",
        "user_prompt = f\"Generate a {selected_mode.lower()} story. The story must have exactly 5 to 7 scenes, \" \\\n",
        "              \"each starting with 'Scene N: ' followed by the scene's description.\" \\\n",
        "              \"Ensure the story is unique and captivating for a short video narration.\"\n",
        "\n",
        "try:\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt,\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.7, # A good balance for creativity and consistency\n",
        "        max_tokens=1500, # Sufficient tokens for 5-7 scenes\n",
        "    )\n",
        "\n",
        "    generated_story_content = chat_completion.choices[0].message.content\n",
        "\n",
        "    # --- 6. Print the generated story to the console ---\n",
        "    print(\"\\n--- Generated Story ---\")\n",
        "    print(generated_story_content)\n",
        "    print(\"\\n-----------------------\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during API call: {e}\")\n",
        "    print(\"Please ensure your GROQ_API_KEY is correct and the Groq API is accessible.\")\n"
      ],
      "metadata": {
        "id": "AeirNEPYaBhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Scene Parsing and Image Prompt Generation\n",
        "\n",
        "The generated story is structured into multiple scenes.  \n",
        "Each scene is parsed and converted into a detailed image prompt to guide the image generation model.\n",
        "\n",
        "The image prompts include:\n",
        "- Environment and setting\n",
        "- Mood and lighting\n",
        "- Cultural and visual elements\n",
        "\n",
        "This step ensures visual consistency between the story and generated images."
      ],
      "metadata": {
        "id": "N1a815zoDH0T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c950fa09"
      },
      "source": [
        "import re\n",
        "\n",
        "# 1. Accepts the generated story as a single multiline string variable.\n",
        "# This variable `generated_story_content` is assumed to be available from the previous step's execution.\n",
        "\n",
        "# 2. Parses the story and extracts each scene separately.\n",
        "# First, find the actual story content section, as there might be metadata/reasoning before it.\n",
        "story_start_marker = \"### Story\"\n",
        "story_section_index = generated_story_content.find(story_start_marker)\n",
        "\n",
        "if story_section_index != -1:\n",
        "    # If the marker is found, take content from there onwards\n",
        "    story_content_only = generated_story_content[story_section_index:]\n",
        "else:\n",
        "    # Otherwise, assume the whole content is the story\n",
        "    story_content_only = generated_story_content\n",
        "\n",
        "# Regex to extract scenes. It captures the scene number and the scene description.\n",
        "# re.DOTALL ensures '.' matches newlines as well.\n",
        "# The positive lookahead `(?=(?:\\*\\*?Scene \\d+:\\*\\*?|$))` ensures it matches up to the next scene or end of string.\n",
        "# Updated regex to handle '**Scene N:**' format as well as 'Scene N:'\n",
        "scenes_raw = re.findall(r'\\*\\*?Scene (\\d+):\\*\\*?\\s*(.+?)(?=(?:\\*\\*?Scene \\d+:\\*\\*?|$))', story_content_only, re.DOTALL)\n",
        "\n",
        "parsed_scenes = []\n",
        "for scene_num_str, scene_text in scenes_raw:\n",
        "    parsed_scenes.append({\n",
        "        \"scene_number\": int(scene_num_str),\n",
        "        \"description\": scene_text.strip()\n",
        "    })\n",
        "\n",
        "# 3. For each scene, generate a detailed image-generation prompt.\n",
        "# 4. Each image prompt must follow this structure:\n",
        "#    Image Prompt for Scene X:\n",
        "#    <detailed visual description>\n",
        "\n",
        "image_prompts = []\n",
        "for scene in parsed_scenes:\n",
        "    scene_number = scene[\"scene_number\"]\n",
        "    scene_description = scene[\"description\"]\n",
        "\n",
        "    # Construct a detailed image prompt.\n",
        "    # Adhering to \"Do NOT use LLM APIs here\", this prompt is constructed programmatically\n",
        "    # based on a template and the raw scene text.\n",
        "    # It aims to include visual representation, environment, mood, lighting, and cultural elements\n",
        "    # as instructed, suitable for Stable Diffusion / Flux, and avoiding text/watermarks.\n",
        "    image_prompt_detail = (\n",
        "        f\"A visually stunning and detailed digital painting, cinematic storybook art style, \"\n",
        "        f\"depicting: {scene_description}. \"\n",
        "        f\"Emphasize the environment and atmosphere mentioned. \"\n",
        "        f\"Mood is evocative and culturally rich. \"\n",
        "        f\"Dramatic, warm, or mystical lighting as appropriate to the scene. \"\n",
        "        f\"High resolution, intricate details, vibrant colors, fantasy realism. \"\n",
        "        f\"No text, no captions, no watermarks.\"\n",
        "    )\n",
        "\n",
        "    image_prompts.append({\n",
        "        \"scene_number\": scene_number,\n",
        "        \"image_prompt\": image_prompt_detail\n",
        "    })\n",
        "\n",
        "# 5. Store all image prompts in a Python list or dictionary.\n",
        "#    Already stored in the `image_prompts` list of dictionaries.\n",
        "\n",
        "# 6. Print all image prompts clearly to the console.\n",
        "print(\"\\n--- Generated Image Prompts ---\")\n",
        "for prompt_data in image_prompts:\n",
        "    print(f\"Image Prompt for Scene {prompt_data['scene_number']}:\")\n",
        "    print(prompt_data['image_prompt'])\n",
        "    print(\"-\" * 30) # Separator for clarity\n",
        "print(\"-----------------------------\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Image Generation Using Stable Diffusion XL\n",
        "\n",
        "For each scene-wise image prompt, images are generated using a pre-trained diffusion model.  \n",
        "Stable Diffusion XL (SDXL) is used to generate high-quality, visually rich images.\n",
        "\n",
        "The model converts text-based prompts into images without requiring any model training.  \n",
        "Each generated image corresponds to one scene in the story and is saved locally for further processing."
      ],
      "metadata": {
        "id": "Z_IR0hIeDJ35"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "169b8a61"
      },
      "source": [
        "# Install necessary libraries\n",
        "!pip install -qqq diffusers transformers accelerate groq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "149cb475"
      },
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "import os\n",
        "from IPython.display import display\n",
        "\n",
        "# 1. Ensure output directory exists\n",
        "OUTPUT_DIR = \"output\"\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "# 2. Load pre-trained Stable Diffusion XL model\n",
        "# This will automatically use the GPU if available and CUDA is set up\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True\n",
        ").to(\"cuda\")\n",
        "\n",
        "print(\"Stable Diffusion XL model loaded successfully.\")\n",
        "\n",
        "# 3. For each image prompt, generate one high-quality image\n",
        "# and save it locally\n",
        "\n",
        "# Assuming image_prompts is a list of dictionaries with 'image_prompt' and 'scene_number'\n",
        "# from the previous step. Example: [{'scene_number': 1, 'image_prompt': '...'}]\n",
        "\n",
        "for prompt_data in image_prompts:\n",
        "    scene_number = prompt_data['scene_number']\n",
        "    image_prompt = prompt_data['image_prompt']\n",
        "\n",
        "    print(f\"Generating image for Scene {scene_number}...\")\n",
        "\n",
        "    # Generate image\n",
        "    # cinematic, storybook-style visuals\n",
        "    # Ensure no text, captions, or watermarks appear in the image\n",
        "    # These are general guidance, specific negative prompts might be needed for perfect results\n",
        "    negative_prompt = \"text, watermark, signature, caption, logo, blurry, low resolution, bad anatomy, deformed, disfigured\"\n",
        "\n",
        "    image = pipe(\n",
        "        prompt=image_prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_inference_steps=30, # A good balance between quality and speed\n",
        "        guidance_scale=7.5 # Controls adherence to the prompt\n",
        "    ).images[0]\n",
        "\n",
        "    # 4. Save each generated image locally\n",
        "    image_filename = os.path.join(OUTPUT_DIR, f\"scene_{scene_number}.png\")\n",
        "    image.save(image_filename)\n",
        "    print(f\"Image saved: {image_filename}\")\n",
        "\n",
        "    # 5. Display each generated image in the notebook\n",
        "    print(f\"Displaying image for Scene {scene_number}:\")\n",
        "    display(image)\n",
        "    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
        "\n",
        "print(\"Image generation complete for all scenes!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Audio Generation Using Text-to-Speech\n",
        "\n",
        "After generating the story text, all scenes are combined into a single narration script.  \n",
        "This narration text is converted into natural-sounding speech using a free neural text-to-speech engine.\n",
        "\n",
        "Text-to-speech enables the story to be presented in an audio format, improving accessibility and engagement.  \n",
        "The generated narration audio is saved as a single audio file and later synchronized with images in the video creation step."
      ],
      "metadata": {
        "id": "amy5KQ_7DSx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install Edge TTS library\n",
        "!pip install -qqq edge-tts\n",
        "\n",
        "import os\n",
        "import edge_tts\n",
        "import asyncio\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# --- 1. Combine all scenes into a single narration text ---\n",
        "# Assuming 'parsed_scenes' is available from the previous step,\n",
        "# which contains a list of dictionaries, each with a 'description' key.\n",
        "\n",
        "narration_text = \"\"\n",
        "if 'parsed_scenes' in locals() and parsed_scenes:\n",
        "    for scene in parsed_scenes:\n",
        "        narration_text += scene['description'] + \"\\n\\n\"\n",
        "    print(\"\\n--- Combined Narration Text ---\")\n",
        "    print(narration_text.strip())\n",
        "    print(\"-----------------------------\")\n",
        "else:\n",
        "    print(\"Error: 'parsed_scenes' not found or empty. Please ensure previous steps ran correctly.\")\n",
        "    narration_text = \"Default narration text. Please run previous cells to generate actual story.\"\n",
        "\n",
        "# --- 2. Convert the narration text into natural-sounding speech using Edge TTS ---\n",
        "# --- 3. Use Edge TTS (Microsoft Edge Text-to-Speech) with a natural voice ---\n",
        "\n",
        "async def generate_and_play_audio(text, voice_name, output_file):\n",
        "    print(f\"\\nGenerating audio for narration using voice: {voice_name}...\")\n",
        "    try:\n",
        "        # Create a Communicate object\n",
        "        communicate = edge_tts.Communicate(text, voice_name)\n",
        "\n",
        "        # Save the audio to a file\n",
        "        await communicate.save(output_file)\n",
        "        print(f\"Audio saved to {output_file}\")\n",
        "\n",
        "        # --- 6. Play the generated audio inside the notebook ---\n",
        "        print(\"Playing generated audio:\")\n",
        "        display(Audio(output_file, autoplay=False))\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during audio generation or playback: {e}\")\n",
        "\n",
        "# Define the output filename\n",
        "output_audio_file = \"narration.mp3\"\n",
        "\n",
        "# Choose a natural voice (e.g., 'en-US-AriaNeural' or 'en-IN-PrabhatNeural')\n",
        "# You can find available voices using `edge-tts --list-voices` in a terminal.\n",
        "selected_voice = \"en-US-AriaNeural\"\n",
        "\n",
        "# Run the asynchronous function\n",
        "# This is how you run an async function in a Jupyter/Colab environment\n",
        "await generate_and_play_audio(narration_text, selected_voice, output_audio_file)\n",
        "\n",
        "print(\"\\nAudio generation and playback complete!\")\n"
      ],
      "metadata": {
        "id": "Ea0Y3KSQn1Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Video Creation and Caption Overlay\n",
        "\n",
        "In the final step, all generated images and narration audio are combined into a single video.  \n",
        "Each image is displayed sequentially, and the narration audio plays continuously in the background.\n",
        "\n",
        "Scene-wise captions are automatically overlaid on the video using the corresponding scene text.  \n",
        "The duration of each scene is calculated based on the total audio duration, ensuring proper synchronization between visuals, captions, and narration.\n",
        "\n",
        "This step completes the end-to-end automated storytelling pipeline."
      ],
      "metadata": {
        "id": "02DjccfLQv0Y"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47d7fe21"
      },
      "source": [
        "# Install MoviePy library and Pillow for text rendering\n",
        "!pip install -qqq moviepy Pillow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e209143"
      },
      "source": [
        "from moviepy.editor import ImageClip, AudioFileClip, concatenate_videoclips\n",
        "import os\n",
        "from IPython.display import Video, display\n",
        "\n",
        "# Define output video filename\n",
        "FINAL_VIDEO_FILENAME = \"final_story_video.mp4\"\n",
        "VIDEO_RESOLUTION = (1280, 720) # Standard HD resolution\n",
        "\n",
        "# 1. Load all scene images in the correct order.\n",
        "# Assuming images are named scene_1.png, scene_2.png, etc., and located in OUTPUT_DIR\n",
        "image_files = sorted([f for f in os.listdir(OUTPUT_DIR) if f.startswith('scene_') and f.endswith('.png')])\n",
        "image_paths = [os.path.join(OUTPUT_DIR, f) for f in image_files]\n",
        "\n",
        "if not image_paths:\n",
        "    raise FileNotFoundError(f\"No scene images found in {OUTPUT_DIR}. Please ensure images were generated correctly.\")\n",
        "\n",
        "# 2. Load the narration audio file.\n",
        "narration_audio = AudioFileClip(\"narration.mp3\")\n",
        "\n",
        "# 3. Automatically calculate the duration for each image.\n",
        "total_audio_duration = narration_audio.duration\n",
        "num_images = len(image_paths)\n",
        "duration_per_image = total_audio_duration / num_images\n",
        "\n",
        "print(f\"Total audio duration: {total_audio_duration:.2f} seconds\")\n",
        "print(f\"Number of images: {num_images}\")\n",
        "print(f\"Duration per image: {duration_per_image:.2f} seconds\\n\")\n",
        "\n",
        "# Create ImageClips for each scene\n",
        "image_clips = []\n",
        "for i, img_path in enumerate(image_paths):\n",
        "    clip = ImageClip(img_path, duration=duration_per_image)\n",
        "    clip = clip.resize(VIDEO_RESOLUTION) # Resize to a standard resolution\n",
        "    image_clips.append(clip)\n",
        "    print(f\"Created clip for {os.path.basename(img_path)} with duration {duration_per_image:.2f}s\")\n",
        "\n",
        "# 4. Create a video by concatenating image clips\n",
        "final_video_clip = concatenate_videoclips(image_clips, method=\"compose\")\n",
        "\n",
        "# 5. Set the narration audio to play continuously in the background\n",
        "final_video_clip = final_video_clip.set_audio(narration_audio)\n",
        "\n",
        "# Export the final video\n",
        "print(f\"\\nExporting final video as {FINAL_VIDEO_FILENAME}...\")\n",
        "final_video_clip.write_videofile(\n",
        "    FINAL_VIDEO_FILENAME,\n",
        "    fps=24, # Frames per second for the video output\n",
        "    codec='libx264', # H.264 codec for good compatibility\n",
        "    audio_codec='aac', # AAC audio codec\n",
        "    remove_temp=True # Clean up temporary files\n",
        ")\n",
        "\n",
        "print(\"Video creation complete!\")\n",
        "\n",
        "# 6. Display the generated video inside the notebook for verification.\n",
        "print(f\"\\nDisplaying generated video: {FINAL_VIDEO_FILENAME}\")\n",
        "display(Video(FINAL_VIDEO_FILENAME, embed=True, width=VIDEO_RESOLUTION[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import ImageClip, AudioFileClip, TextClip, CompositeVideoClip, ColorClip, concatenate_videoclips\n",
        "from moviepy.config import change_settings\n",
        "import os\n",
        "from IPython.display import Video, display\n",
        "from PIL import Image, ImageDraw, ImageFont # Import Pillow libraries\n",
        "\n",
        "# Configure MoviePy (ImageMagick path setting is no longer needed for text rendering with Pillow)\n",
        "# change_settings({\"IMAGEMAGICK_BINARY\": r\"/usr/bin/convert\"}) # Commented out as TextClip is no longer used for text rendering\n",
        "\n",
        "# Define output video filename and resolution\n",
        "FINAL_VIDEO_FILENAME_CAPTIONS = \"final_story_video_with_captions.mp4\"\n",
        "VIDEO_RESOLUTION = (1280, 720) # Standard HD resolution\n",
        "\n",
        "# Assuming OUTPUT_DIR is defined from previous image generation step\n",
        "OUTPUT_DIR = \"output\"\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    print(f\"Warning: Output directory {OUTPUT_DIR} not found. Ensure images are generated.\")\n",
        "\n",
        "# 1. Load all scene images in the correct order.\n",
        "image_files = sorted([f for f in os.listdir(OUTPUT_DIR) if f.startswith('scene_') and f.endswith('.png')])\n",
        "image_paths = [os.path.join(OUTPUT_DIR, f) for f in image_files]\n",
        "\n",
        "if not image_paths:\n",
        "    raise FileNotFoundError(f\"No scene images found in {OUTPUT_DIR}. Please ensure images were generated correctly.\")\n",
        "\n",
        "# 2. Load the narration audio file.\n",
        "narration_audio = AudioFileClip(\"narration.mp3\")\n",
        "\n",
        "# 3. Load the scene-wise text content to be used as captions.\n",
        "# Assuming 'parsed_scenes' is available from the step that parsed the story.\n",
        "# It should be a list of dictionaries, e.g., [{'scene_number': 1, 'description': '...'}]\n",
        "if 'parsed_scenes' not in locals() or not parsed_scenes:\n",
        "    raise ValueError(\n",
        "        \"'parsed_scenes' variable not found or is empty. \"\n",
        "        \"Please ensure the story parsing step (e.g., cell c950fa09) was executed correctly.\"\n",
        "    )\n",
        "\n",
        "# 4. Calculate the duration for each scene.\n",
        "total_audio_duration = narration_audio.duration\n",
        "num_scenes = len(parsed_scenes)\n",
        "duration_per_scene = total_audio_duration / num_scenes\n",
        "\n",
        "print(f\"Total audio duration: {total_audio_duration:.2f} seconds\")\n",
        "print(f\"Number of scenes: {num_scenes}\")\n",
        "print(f\"Duration per scene (approx): {duration_per_scene:.2f} seconds\\n\")\n",
        "\n",
        "video_clips_with_captions = []\n",
        "for i, scene_data in enumerate(parsed_scenes):\n",
        "    scene_number = scene_data['scene_number']\n",
        "    scene_description = scene_data['description']\n",
        "    img_path = image_paths[i] # Assuming image_paths are sorted and match scene order\n",
        "\n",
        "    print(f\"Processing Scene {scene_number}: {scene_description[:50]}...\")\n",
        "\n",
        "    # 5. Create an ImageClip for the scene image.\n",
        "    image_clip = ImageClip(img_path, duration=duration_per_scene)\n",
        "    image_clip = image_clip.resize(VIDEO_RESOLUTION) # Resize to standard resolution\n",
        "\n",
        "    # --- NEW: Generate caption as an image using Pillow ----\n",
        "    # Define text properties\n",
        "    font_size = 30\n",
        "    # Use a common font available in Colab environments, e.g., DejaVuSans-Bold\n",
        "    font_path = \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\"\n",
        "    try:\n",
        "        font = ImageFont.truetype(font_path, font_size)\n",
        "    except IOError:\n",
        "        print(f\"Warning: Font not found at {font_path}, using default Pillow font.\")\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    max_text_width_pixels = int(VIDEO_RESOLUTION[0] * 0.9) # Max width 90% of video width\n",
        "    padding = 20 # Padding around the text\n",
        "\n",
        "    # Word wrap the text to fit within the max_text_width_pixels\n",
        "    lines = []\n",
        "    current_line = []\n",
        "\n",
        "    # Use a temporary ImageDraw object to measure text length for wrapping\n",
        "    temp_img_measure = Image.new('RGB', (1,1))\n",
        "    temp_draw_measure = ImageDraw.Draw(temp_img_measure)\n",
        "\n",
        "    words = scene_description.split(' ')\n",
        "    for word in words:\n",
        "        test_line = ' '.join(current_line + [word])\n",
        "        if temp_draw_measure.textlength(test_line, font=font) <= max_text_width_pixels:\n",
        "            current_line.append(word)\n",
        "        else:\n",
        "            if current_line:\n",
        "                lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "    if current_line: # Add any remaining words as the last line\n",
        "        lines.append(' '.join(current_line))\n",
        "\n",
        "    wrapped_text = \"\\n\".join(lines)\n",
        "\n",
        "    # Get the bounding box for the entire wrapped text to determine image size\n",
        "    text_bbox = temp_draw_measure.textbbox((0, 0), wrapped_text, font=font)\n",
        "    actual_text_width = text_bbox[2] - text_bbox[0]\n",
        "    actual_text_height = text_bbox[3] - text_bbox[1]\n",
        "\n",
        "    caption_img_width = actual_text_width + padding * 2\n",
        "    caption_img_height = actual_text_height + padding * 2\n",
        "\n",
        "    # Ensure minimum size to prevent issues with very short text\n",
        "    if caption_img_width < 10: caption_img_width = 100\n",
        "    if caption_img_height < 10: caption_img_height = 50\n",
        "\n",
        "    # Create a transparent Pillow image for the caption\n",
        "    text_image = Image.new('RGBA', (caption_img_width, caption_img_height), (0, 0, 0, 0))\n",
        "    draw = ImageDraw.Draw(text_image)\n",
        "\n",
        "    # Draw semi-transparent black background rectangle\n",
        "    bg_color = (0, 0, 0, int(0.6 * 255)) # Black with 60% opacity\n",
        "    draw.rectangle([(0, 0), (caption_img_width, caption_img_height)], fill=bg_color)\n",
        "\n",
        "    # Draw the wrapped text in white\n",
        "    draw.text((padding, padding), wrapped_text, font=font, fill=(255, 255, 255, 255))\n",
        "\n",
        "    # Save the Pillow image to a temporary PNG file\n",
        "    temp_caption_file = f\"temp_caption_scene_{scene_number}.png\"\n",
        "    text_image.save(temp_caption_file)\n",
        "\n",
        "    # Create an ImageClip from the temporary PNG file\n",
        "    text_overlay_clip = ImageClip(temp_caption_file, duration=duration_per_scene)\n",
        "    text_overlay_clip = text_overlay_clip.set_pos(('center', 'bottom')) # Position at bottom center\n",
        "\n",
        "    # Overlay the text image clip on the main image clip\n",
        "    final_scene_clip = CompositeVideoClip([\n",
        "        image_clip,\n",
        "        text_overlay_clip\n",
        "    ])\n",
        "    video_clips_with_captions.append(final_scene_clip)\n",
        "\n",
        "    # Clean up the temporary caption image file\n",
        "    os.remove(temp_caption_file)\n",
        "    # --- END NEW ---\n",
        "\n",
        "# 7. Combine all scene clips sequentially.\n",
        "final_video_clip_with_captions = concatenate_videoclips(video_clips_with_captions, method=\"compose\")\n",
        "\n",
        "# 8. Attach the narration audio to the final video.\n",
        "final_video_clip_with_captions = final_video_clip_with_captions.set_audio(narration_audio)\n",
        "\n",
        "# 9. Export the final video.\n",
        "print(f\"\\nExporting final video as {FINAL_VIDEO_FILENAME_CAPTIONS}...\")\n",
        "final_video_clip_with_captions.write_videofile(\n",
        "    FINAL_VIDEO_FILENAME_CAPTIONS,\n",
        "    fps=24, # Frames per second for the video output\n",
        "    codec='libx264', # H.264 codec for good compatibility\n",
        "    audio_codec='aac', # AAC audio codec\n",
        "    remove_temp=True # Clean up temporary files\n",
        ")\n",
        "\n",
        "print(\"Video creation with captions complete!\")\n",
        "\n",
        "# 10. Display the final video inside the notebook for verification.\n",
        "print(f\"\\nDisplaying generated video: {FINAL_VIDEO_FILENAME_CAPTIONS}\")\n",
        "display(Video(FINAL_VIDEO_FILENAME_CAPTIONS, embed=True, width=VIDEO_RESOLUTION[0]))"
      ],
      "metadata": {
        "id": "A69BQ_xpuRW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Evaluation and Analysis\n",
        "\n",
        "The Smart Cultural Storyteller system was evaluated based on the following criteria:\n",
        "\n",
        "### 5.1 Story Quality\n",
        "- Stories generated are unique for each execution due to controlled randomness in the LLM.\n",
        "- Scene-wise structure improves clarity and multimedia mapping.\n",
        "- Different storytelling modes (mythological, cultural, emotional, ancestral) produce distinct narrative styles.\n",
        "\n",
        "### 5.2 Visual Relevance\n",
        "- Generated images closely align with scene descriptions.\n",
        "- Prompt engineering ensures cultural and emotional consistency.\n",
        "- Image quality is high without any manual post-processing.\n",
        "\n",
        "### 5.3 Audio Clarity\n",
        "- Text-to-speech narration is clear and natural.\n",
        "- Audio pacing synchronizes well with visual scene transitions.\n",
        "\n",
        "### 5.4 Overall System Performance\n",
        "- The pipeline runs fully automatically.\n",
        "- No model training is required.\n",
        "- Output video is generated successfully in a single execution.\n",
        "\n",
        "The results demonstrate the effectiveness of integrating multiple AI tools into a unified storytelling pipeline."
      ],
      "metadata": {
        "id": "-sPG1XDTRHWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Ethical Considerations and Responsible AI\n",
        "\n",
        "The system is designed with ethical AI principles in mind:\n",
        "\n",
        "- Only pre-trained and licensed models are used.\n",
        "- No personal or sensitive user data is collected or stored.\n",
        "- Generated cultural and mythological content avoids offensive or biased representations.\n",
        "- The system does not attempt to replace human creativity but acts as an assistive storytelling tool.\n",
        "\n",
        "Clear disclaimers can be included in future deployments to indicate that the content is AI-generated."
      ],
      "metadata": {
        "id": "gHGUUYoeRIml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Conclusion and Future Scope\n",
        "\n",
        "This project successfully demonstrates an end-to-end automated storytelling system that generates scripts, visuals, audio, and video output using existing AI tools.\n",
        "\n",
        "### 7.1 Conclusion\n",
        "- The system integrates LLMs, diffusion models, and text-to-speech engines effectively.\n",
        "- It produces engaging multimodal storytelling content without training any models.\n",
        "- The modular pipeline allows easy replacement or upgrade of individual components.\n",
        "\n",
        "### 7.2 Future Scope\n",
        "- User-specific personalization (language, voice, cultural region)\n",
        "- Support for regional languages\n",
        "- Real-time video generation\n",
        "- Web or mobile app deployment\n",
        "- Use of advanced video generation models\n",
        "\n",
        "The Smart Cultural Storyteller has strong potential for applications in education, digital heritage preservation, and entertainment."
      ],
      "metadata": {
        "id": "4pHR_H7qRQtl"
      }
    }
  ]
}